{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "QB3WEyIVstl0"
            },
            "source": [
                "# 5章\n",
                "- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "kvqSUAEtU_VJ"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (4.40.1)\n",
                        "Requirement already satisfied: fugashi in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (1.3.2)\n",
                        "Requirement already satisfied: ipadic in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (1.0.0)\n",
                        "Requirement already satisfied: filelock in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
                        "Requirement already satisfied: numpy>=1.17 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (24.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (2024.4.28)\n",
                        "Requirement already satisfied: requests in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
                        "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.1 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryohei.yamamoto/projects/ml-playground/venv/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# 5-1\n",
                "%pip install transformers fugashi ipadic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "DWT32lOgHLrU"
            },
            "outputs": [],
            "source": [
                "# 5-2\n",
                "import numpy as np\n",
                "import torch\n",
                "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "I7X-Iy52AC1v"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "# 5-3\n",
                "# tohoku-nlp/bert-base-japanese-whole-word-masking https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking\n",
                "model_name = 'tohoku-nlp/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
                "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
                "# bert_mlm = bert_mlm.cuda()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "EfKt-j0WLOfx"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['今日', 'は', '[MASK]', 'へ', '行く', '。']\n"
                    ]
                }
            ],
            "source": [
                "# 5-4\n",
                "text = '今日は[MASK]へ行く。'\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "YaW5Y9fM5zeM"
            },
            "outputs": [],
            "source": [
                "# 5-5\n",
                "# 文章を符号化し、GPUに配置する。\n",
                "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
                "# input_ids = input_ids.cuda()\n",
                "\n",
                "# BERTに入力し、分類スコアを得る。\n",
                "# 系列長を揃える必要がないので、単にiput_idsのみを入力します。\n",
                "with torch.no_grad():\n",
                "    output = bert_mlm(input_ids=input_ids)\n",
                "    scores = output.logits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "id": "Z-5lnX9r8XKl"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[ -5.8525,   5.0457,  -1.7965,  ...,  -4.8386,  -6.4219,  -7.8085],\n",
                        "        [ -4.0218,   7.2845,  -5.3993,  ...,  -6.0369,  -6.5811,  -2.1289],\n",
                        "        [ -5.8364,   5.3642,  -2.2106,  ...,  -4.3529,  -5.7284,  -4.3889],\n",
                        "        ...,\n",
                        "        [ -7.8698,   5.9753,  -4.3923,  ...,  -4.3223,  -6.0900, -11.4386],\n",
                        "        [ -5.4500,   6.5491,   0.0368,  ...,  -4.5615,  -5.1636,  -7.0161],\n",
                        "        [ -8.7510,   3.2686,  -1.6596,  ...,  -5.0593,  -7.0547, -10.7624]])\n",
                        "scores[0, mask_position].argmax(-1): <class 'torch.Tensor'> scores[0, mask_position].argmax(-1).item(): 391\n",
                        "今日は東京へ行く。\n"
                    ]
                }
            ],
            "source": [
                "# 5-6\n",
                "# ID列で'[MASK]'(IDは4)の位置を調べる\n",
                "mask_position = input_ids[0].tolist().index(4) \n",
                "\n",
                "# スコアが最も良いトークンのIDを取り出し、トークンに変換する。\n",
                "print(scores[0])\n",
                "# argmaxとは、最大値のインデックスを返す関数. -1は最後の次元を指定している。\n",
                "# .item()は、Tensorをスカラー値に変換する関数。\n",
                "print(f\"scores[0, mask_position].argmax(-1): {type(scores[0, mask_position].argmax(-1))} scores[0, mask_position].argmax(-1).item(): {scores[0, mask_position].argmax(-1).item()}\")\n",
                "id_best = scores[0, mask_position].argmax(-1).item()\n",
                "token_best = tokenizer.convert_ids_to_tokens(id_best)\n",
                "token_best = token_best.replace('##', '')\n",
                "\n",
                "# [MASK]を上で求めたトークンで置き換える。\n",
                "text = text.replace('[MASK]',token_best)\n",
                "\n",
                "print(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "id": "TgbIA-1-EVaJ"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "今日は東京へ行く。\n",
                        "今日はハワイへ行く。\n",
                        "今日は学校へ行く。\n",
                        "今日はニューヨークへ行く。\n",
                        "今日はどこへ行く。\n",
                        "今日は空港へ行く。\n",
                        "今日はアメリカへ行く。\n",
                        "今日は病院へ行く。\n",
                        "今日はそこへ行く。\n",
                        "今日はロンドンへ行く。\n"
                    ]
                }
            ],
            "source": [
                "# 5-7\n",
                "def predict_mask_topk(text, tokenizer, bert_mlm, num_topk):\n",
                "    \"\"\"\n",
                "    文章中の最初の[MASK]をスコアの上位のトークンに置き換える。\n",
                "    上位何位まで使うかは、num_topkで指定。\n",
                "    出力は穴埋めされた文章のリストと、置き換えられたトークンのスコアのリスト。\n",
                "    \"\"\"\n",
                "    # 文章を符号化し、BERTで分類スコアを得る。\n",
                "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
                "    # input_ids = input_ids.cuda()\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(input_ids=input_ids)\n",
                "    scores = output.logits\n",
                "\n",
                "    # スコアが上位のトークンとスコアを求める。\n",
                "    mask_position = input_ids[0].tolist().index(4) \n",
                "    topk = scores[0, mask_position].topk(num_topk)\n",
                "    ids_topk = topk.indices # トークンのID\n",
                "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk) # トークン\n",
                "    scores_topk = topk.values.cpu().numpy() # スコア\n",
                "\n",
                "    # 文章中の[MASK]を上で求めたトークンで置き換える。\n",
                "    text_topk = [] # 穴埋めされたテキストを追加する。\n",
                "    for token in tokens_topk:\n",
                "        token = token.replace('##', '')\n",
                "        text_topk.append(text.replace('[MASK]', token, 1))\n",
                "\n",
                "    return text_topk, scores_topk\n",
                "\n",
                "text = '今日は[MASK]へ行く。'\n",
                "text_topk, _ = predict_mask_topk(text, tokenizer, bert_mlm, 10)\n",
                "print(*text_topk, sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "id": "yCaGV_rT3A5N"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'今日は、東京へ行く。'"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 5-8\n",
                "def greedy_prediction(text, tokenizer, bert_mlm):\n",
                "    \"\"\"\n",
                "    [MASK]を含む文章を入力として、貪欲法で穴埋めを行った文章を出力する。\n",
                "    \"\"\"\n",
                "    # 前から順に[MASK]を一つづつ、スコアの最も高いトークンに置き換える。\n",
                "    for _ in range(text.count('[MASK]')):\n",
                "        text = predict_mask_topk(text, tokenizer, bert_mlm, 1)[0][0]\n",
                "    return text\n",
                "\n",
                "text = '今日は[MASK][MASK]へ行く。'\n",
                "greedy_prediction(text, tokenizer, bert_mlm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "id": "prdEvsxBrrGq"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'今日は社会社会的な地位'"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 5-9\n",
                "text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
                "# 貪欲法はMASKが増えると自然な文章にならない。\n",
                "# これはBERTがごく一部のMASKを置き換えて周りの文脈からトークンを予測するタスクを用いているため\n",
                "greedy_prediction(text, tokenizer, bert_mlm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "id": "yHRemOdN0QE9"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "今日はお台場へ行く。\n",
                        "今日はお祭りへ行く。\n",
                        "今日はゲームセンターへ行く。\n",
                        "今日はお風呂へ行く。\n",
                        "今日はゲームショップへ行く。\n",
                        "今日は東京ディズニーランドへ行く。\n",
                        "今日はお店へ行く。\n",
                        "今日は同じ場所へ行く。\n",
                        "今日はあの場所へ行く。\n",
                        "今日は同じ学校へ行く。\n"
                    ]
                }
            ],
            "source": [
                "# 5-10\n",
                "def beam_search(text, tokenizer, bert_mlm, num_topk):\n",
                "    \"\"\"\n",
                "    ビームサーチで文章の穴埋めを行う。\n",
                "    \"\"\"\n",
                "    num_mask = text.count('[MASK]')\n",
                "    text_topk = [text]\n",
                "    scores_topk = np.array([0])\n",
                "    for _ in range(num_mask):\n",
                "        # 現在得られている、それぞれの文章に対して、\n",
                "        # 最初の[MASK]をスコアが上位のトークンで穴埋めする。\n",
                "        text_candidates = [] # それぞれの文章を穴埋めした結果を追加する。\n",
                "        score_candidates = [] # 穴埋めに使ったトークンのスコアを追加する。\n",
                "        for text_mask, score in zip(text_topk, scores_topk):\n",
                "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
                "                text_mask, tokenizer, bert_mlm, num_topk\n",
                "            )\n",
                "            # extendでリストを結合する。\n",
                "            text_candidates.extend(text_topk_inner)\n",
                "            score_candidates.append( score + scores_topk_inner )\n",
                "\n",
                "        # 穴埋めにより生成された文章の中から合計スコアの高いものを選ぶ。\n",
                "        # np.hstackで2次元配列を1次元配列に変換する。\n",
                "        score_candidates = np.hstack(score_candidates)\n",
                "        # argsort()は、要素を昇順に並び替えた際のインデックスを返す。[::-1]で降順に並び替える。[:num_topk]で上位num_topk個を取得する。\n",
                "        idx_list = score_candidates.argsort()[::-1][:num_topk]\n",
                "        # text_topkとscore_topkを更新し、次の[MASK]に進むことで、ビームサーチを繰り返し、より良い文章を生成する。\n",
                "        text_topk = [ text_candidates[idx] for idx in idx_list ]\n",
                "        scores_topk = score_candidates[idx_list]\n",
                "\n",
                "    return text_topk\n",
                "\n",
                "text = \"今日は[MASK][MASK]へ行く。\"\n",
                "text_topk = beam_search(text, tokenizer, bert_mlm, 10)\n",
                "print(*text_topk, sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "id": "5mhL-VSTvUo7"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "今日は社会社会学会所属。\n",
                        "今日は社会社会学会会長。\n",
                        "今日は社会社会に属する。\n",
                        "今日は時代社会に属する。\n",
                        "今日は社会社会学会理事。\n",
                        "今日は時代社会にあたる。\n",
                        "今日は社会社会にある。\n",
                        "今日は社会社会学会会員。\n",
                        "今日は時代社会にある。\n",
                        "今日は社会社会になる。\n"
                    ]
                }
            ],
            "source": [
                "# 5-11\n",
                "text = '今日は[MASK][MASK][MASK][MASK][MASK]'\n",
                "text_topk = beam_search(text, tokenizer, bert_mlm, 10)\n",
                "# ビームサーチもMASKが増えると自然な文書ではなくなる。\n",
                "print(*text_topk, sep='\\n')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Chapter05.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
